{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f45OF7cEMztZ",
    "outputId": "a6d1e2bf-2af6-4aab-85e9-47326b0f5b42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a0a9782a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "#from google.colab import drive\n",
    "import pickle\n",
    "import random\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonTA5s0QEg_"
   },
   "source": [
    "## **Environment Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rKAWc2E9BHIN"
   },
   "outputs": [],
   "source": [
    "class BanditEnvironment:\n",
    "    def __init__(self, difficulty, max_steps = 100):\n",
    "        self.num_actions = 2  # Number of actions (arms/bandits)\n",
    "        self.difficulty = difficulty\n",
    "        self.max_steps = max_steps  # Maximum number of steps per episode\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "\n",
    "        # Set brenoulli probabilities for difficulties\n",
    "        if self.difficulty == 'easy':\n",
    "            bandits_prob = np.random.choice([0.9, 0.1])\n",
    "            self.bandits_prob = np.array([bandits_prob, 1 - bandits_prob])\n",
    "        elif self.difficulty == 'medium':\n",
    "            bandits_prob = np.random.choice([0.75, 0.25])\n",
    "            self.bandits_prob = np.array([bandits_prob, 1 - bandits_prob])\n",
    "        elif self.difficulty == 'hard':\n",
    "            bandits_prob = np.random.choice([0.6, 0.4])\n",
    "            self.bandits_prob = np.array([bandits_prob, 1 - bandits_prob])\n",
    "        elif self.difficulty == 'uniform':\n",
    "            bandits_prob = np.random.uniform()\n",
    "            self.bandits_prob = np.array([bandits_prob, 1 - bandits_prob])\n",
    "        else:  # Independent difficulty (random probabilities for each arm)\n",
    "            self.bandits_prob = np.random.uniform(size=2)\n",
    "\n",
    "        self.timestep = 0  # Initialize timestep\n",
    "        self.last_action = None\n",
    "        self.last_reward = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, and done flag.\"\"\"\n",
    "        self.timestep += 1\n",
    "        self.last_action = action\n",
    "        bandit_prob = self.bandits_prob[action]\n",
    "        reward = 1 if np.random.uniform() < bandit_prob else 0\n",
    "        self.last_reward = reward\n",
    "        done = self.timestep >= self.max_steps  # End episode after 100 timesteps\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return the current state: timestep, last action, and last reward.\"\"\"\n",
    "        one_hot_action = np.zeros(self.num_actions)\n",
    "        if self.last_action is not None:\n",
    "          one_hot_action[self.last_action] = 1\n",
    "        return np.concatenate([np.array([self.timestep, self.last_reward], dtype=np.float32), one_hot_action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0GTo3kg-SsQI"
   },
   "outputs": [],
   "source": [
    "def metrics(total_reward, bandits_prob, actions_chosen):\n",
    "\n",
    "    # Cumulative Regret\n",
    "    optimal_reward = np.max(bandits_prob) * 100\n",
    "    cumulative_regret = optimal_reward - total_reward\n",
    "\n",
    "    return cumulative_regret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pIs2kAgvBMXk"
   },
   "outputs": [],
   "source": [
    "class A2C_LSTM_Agent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_actions):\n",
    "        super(A2C_LSTM_Agent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.actor_layer = nn.Linear(hidden_size, num_actions)\n",
    "        self.critic_layer = nn.Linear(hidden_size, 1)\n",
    "        nn.init.xavier_uniform_(self.actor_layer.weight, gain=0.01)\n",
    "        nn.init.xavier_uniform_(self.critic_layer.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output of the last timestep\n",
    "        action_probs = F.softmax(self.actor_layer(lstm_out), dim=-1)\n",
    "        value = self.critic_layer(lstm_out)\n",
    "        return action_probs, value, hidden_state\n",
    "\n",
    "    def init_hidden_state(self, batch_size=1):\n",
    "        # Initialize the LSTM hidden and cell states\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QNF9wpyXM1Xv"
   },
   "outputs": [],
   "source": [
    "def train_agent(TRAIN = True, n_episodes = 25000, gamma = 0.99, learning_rate = 0.001, entropy_decay = False):\n",
    "\n",
    "    stats = {'Episode Returns': [], 'Optimal Action': [], 'Bandits_Probs': [], 'Actions Taken': [], 'Cumulative Regret': [], 'Actions-Chosen List': []}\n",
    "\n",
    "    for episode in tqdm(range(1, n_episodes + 1)):\n",
    "\n",
    "        # Restart Constants, Environmentes and LSTM Hidden-State\n",
    "        actions_chosen_list = [] if TRAIN == False else None\n",
    "        actions_chosen = np.zeros(num_actions)\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "        hidden_state = agent.init_hidden_state()\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # Forward pass\n",
    "            action_probs, value, hidden_state = agent(state, (hidden_state[0].detach(), hidden_state[1].detach()))\n",
    "\n",
    "            # Sample action from probability distribution\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "\n",
    "            # Take action in the environment\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            total_reward += reward\n",
    "\n",
    "            # Prepare next state\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            # Trainning\n",
    "            if TRAIN == True:\n",
    "\n",
    "                # Compute TD target and advantage\n",
    "                with torch.no_grad():\n",
    "                    _, next_value, _ = agent(next_state, hidden_state) #if not done else (None, torch.tensor(0.0), None)\n",
    "                    td_target = reward + gamma * next_value\n",
    "                advantage = td_target - value\n",
    "\n",
    "                # Compute losses\n",
    "                actor_loss = -log_prob * advantage.detach()\n",
    "                critic_loss = F.mse_loss(value, td_target)\n",
    "                entropy_loss = -action_dist.entropy().mean()\n",
    "                entropy_coef = max(0.005, 0.1 * (1 - episode / n_episodes)) if entropy_decay == True else 0.005\n",
    "\n",
    "                loss = actor_loss + 0.05 * critic_loss + entropy_coef * entropy_loss\n",
    "\n",
    "                # Optimize the network\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            else:\n",
    "                actions_chosen_list.append(action.item())\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Add action\n",
    "            actions_chosen[action.item()] += 1\n",
    "\n",
    "        # Cumulative Regret:\n",
    "        cumulative_regret_value = metrics(total_reward, env.bandits_prob, actions_chosen)\n",
    "\n",
    "        #Save Episode stats\n",
    "        stats['Episode Returns'].append(total_reward)\n",
    "        stats['Optimal Action'].append(np.argmax(env.bandits_prob))\n",
    "        stats['Bandits_Probs'].append(env.bandits_prob)\n",
    "        stats['Actions Taken'].append(actions_chosen)\n",
    "        stats['Actions-Chosen List'].append(actions_chosen_list)\n",
    "        stats['Cumulative Regret'].append(metrics(total_reward, env.bandits_prob, actions_chosen))\n",
    "\n",
    "        if len(stats['Cumulative Regret']) >= 100:\n",
    "            mean_last_100_regret = np.mean(stats['Cumulative Regret'][-100:])\n",
    "        else:\n",
    "            mean_last_100_regret = np.mean(stats['Cumulative Regret'])\n",
    "\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Ep {episode}/{n_episodes}, Opt. Action: {np.argmax(env.bandits_prob)}, Reward: {total_reward}, Cumulative-Regret: {cumulative_regret_value}, AVG100-Regret: {mean_last_100_regret}\")\n",
    "\n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rRZa9o42Rqy1"
   },
   "outputs": [],
   "source": [
    "def save_train_stats(stats, model_name, train_difficulty, discount_factor_gamma, learning_rate, entropy_decay):\n",
    "\n",
    "    stats = pd.DataFrame(stats)\n",
    "\n",
    "    stats['model_name'] = model_name\n",
    "    stats['train_difficulty'] = train_difficulty\n",
    "    stats['gamma'] = discount_factor_gamma\n",
    "    stats['lr'] = learning_rate\n",
    "    stats['entropy_decay'] = entropy_decay\n",
    "    stats.to_pickle(google_drive_folder + '/' + 'train_stats' + '/' + model_name + '.pkl')\n",
    "\n",
    "def save_test_stats(stats, model_name, train_difficulty, test_difficulty, discount_factor_gamma, learning_rate, entropy_decay):\n",
    "\n",
    "    stats = pd.DataFrame(stats)\n",
    "\n",
    "    stats['model_name'] = model_name\n",
    "    stats['train_difficulty'] = train_difficulty\n",
    "    stats['gamma'] = discount_factor_gamma\n",
    "    stats['lr'] = learning_rate\n",
    "    stats['test_difficulty'] = test_difficulty\n",
    "    stats['test_difficulty'] = test_difficulty\n",
    "    stats['entropy_decay'] = entropy_decay\n",
    "\n",
    "\n",
    "    model_name = model_name + f'__TestDiff_{test_difficulty}'\n",
    "    stats.to_pickle(google_drive_folder + '/' + 'test_stats' + '/' + model_name + '.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jX1wiHgPM1Va",
    "outputId": "6ea1edc0-3ca9-4501-abc1-bf962137bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 1: GAMMA 0.7 - LR 0.0001 - Entropy Decay False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [14:29<6:17:01,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 1, Reward: 83, Cumulative-Regret: 7.0, AVG100-Regret: 5.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [28:24<5:27:59,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 3.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [41:36<5:03:47,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 1, Reward: 94, Cumulative-Regret: -4.0, AVG100-Regret: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [56:17<5:07:49,  1.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 0, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:09:49<5:38:18,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 0, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:25:12<5:17:44,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 0, Reward: 85, Cumulative-Regret: 5.0, AVG100-Regret: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:41:16<4:42:26,  1.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 1.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:55:49<3:46:24,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 0, Reward: 94, Cumulative-Regret: -4.0, AVG100-Regret: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [2:09:35<3:39:43,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:23:24<3:22:42,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:37:16<3:10:58,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 1, Reward: 84, Cumulative-Regret: 6.0, AVG100-Regret: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:50:51<2:53:44,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 0, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [3:04:33<2:55:37,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [3:18:26<2:26:37,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:32:12<2:15:05,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:45:50<2:02:14,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [3:59:48<1:58:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 1, Reward: 86, Cumulative-Regret: 4.0, AVG100-Regret: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [4:13:36<1:36:17,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [4:27:26<1:22:38,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 0, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:41:08<1:07:52,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 1, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:55:02<1:00:16,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [5:09:06<45:09,  1.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 0, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [5:21:37<23:58,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 0, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [5:33:36<12:30,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:45:34<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 1, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:57<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:56<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:58<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:59<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:59<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 2: GAMMA 0.7 - LR 0.001 - Entropy Decay True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [12:32<5:04:37,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 0, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 2.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [24:42<4:29:22,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 0, Reward: 94, Cumulative-Regret: -4.0, AVG100-Regret: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [36:28<4:20:58,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 0, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 3.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [48:14<4:06:37,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:00:03<3:57:06,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 2.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:12:23<3:53:41,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 0, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 2.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:24:42<3:39:53,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 0, Reward: 82, Cumulative-Regret: 8.0, AVG100-Regret: 2.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:36:48<3:21:29,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 0, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 3.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [1:48:35<3:08:04,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 0, Reward: 96, Cumulative-Regret: -6.0, AVG100-Regret: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:00:22<2:55:58,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 0, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 2.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:12:09<2:44:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 1, Reward: 95, Cumulative-Regret: -5.0, AVG100-Regret: 3.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:23:53<2:33:52,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 0, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 2.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [2:35:41<2:29:55,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 0, Reward: 77, Cumulative-Regret: 13.0, AVG100-Regret: 2.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [2:48:16<2:19:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 4.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:00:37<1:56:30,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 0, Reward: 9, Cumulative-Regret: 81.0, AVG100-Regret: 2.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:12:22<1:45:35,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 1, Reward: 95, Cumulative-Regret: -5.0, AVG100-Regret: 1.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [3:24:05<1:33:23,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 0, Reward: 83, Cumulative-Regret: 7.0, AVG100-Regret: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [3:36:31<1:27:40,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [3:48:43<1:10:44,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:00:29<58:23,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 1, Reward: 85, Cumulative-Regret: 5.0, AVG100-Regret: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:12:13<46:44,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 0, Reward: 66, Cumulative-Regret: 24.0, AVG100-Regret: 5.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [4:23:58<35:09,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 1, Reward: 83, Cumulative-Regret: 7.0, AVG100-Regret: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [4:36:31<25:26,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 1, Reward: 94, Cumulative-Regret: -4.0, AVG100-Regret: 1.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [4:48:30<11:42,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 0, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:00:42<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 0, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:56<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:58<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:58<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:58<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:59<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 3: GAMMA 0.99 - LR 0.0001 - Entropy Decay True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [12:15<4:49:08,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 1, Reward: 70, Cumulative-Regret: 20.0, AVG100-Regret: 38.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [25:12<5:05:37,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 0, Reward: 39, Cumulative-Regret: 51.0, AVG100-Regret: 38.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [38:16<4:49:38,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 0, Reward: 47, Cumulative-Regret: 43.0, AVG100-Regret: 35.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [52:10<4:52:28,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 1, Reward: 43, Cumulative-Regret: 47.0, AVG100-Regret: 39.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:06:10<4:39:12,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 1, Reward: 53, Cumulative-Regret: 37.0, AVG100-Regret: 40.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:19:57<4:08:46,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 1, Reward: 30, Cumulative-Regret: 60.0, AVG100-Regret: 38.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:33:04<3:56:07,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 0, Reward: 70, Cumulative-Regret: 20.0, AVG100-Regret: 39.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:46:10<3:55:01,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 0, Reward: 62, Cumulative-Regret: 28.0, AVG100-Regret: 40.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [2:00:11<3:43:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 1, Reward: 67, Cumulative-Regret: 23.0, AVG100-Regret: 41.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:14:10<3:29:33,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 1, Reward: 58, Cumulative-Regret: 32.0, AVG100-Regret: 36.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:27:20<3:05:35,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 38.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:40:42<3:01:54,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 0, Reward: 14, Cumulative-Regret: 76.0, AVG100-Regret: 39.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [2:54:47<2:48:45,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 0, Reward: 56, Cumulative-Regret: 34.0, AVG100-Regret: 38.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [3:08:09<2:23:48,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 1, Reward: 21, Cumulative-Regret: 69.0, AVG100-Regret: 37.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:21:50<2:20:29,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 1, Reward: 71, Cumulative-Regret: 19.0, AVG100-Regret: 39.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:35:30<1:58:43,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 0, Reward: 44, Cumulative-Regret: 46.0, AVG100-Regret: 38.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [3:48:40<1:45:03,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 0, Reward: 25, Cumulative-Regret: 65.0, AVG100-Regret: 41.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [4:02:36<1:32:37,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 1, Reward: 74, Cumulative-Regret: 16.0, AVG100-Regret: 39.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [4:16:16<1:24:47,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 0, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 37.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:29:48<1:06:26,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 42.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:43:02<56:17,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 1, Reward: 7, Cumulative-Regret: 83.0, AVG100-Regret: 38.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [4:56:51<39:25,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 34.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [5:10:22<28:22,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 1, Reward: 11, Cumulative-Regret: 79.0, AVG100-Regret: 44.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [5:24:30<13:55,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 0, Reward: 97, Cumulative-Regret: -7.0, AVG100-Regret: 41.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:37:44<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 0, Reward: 96, Cumulative-Regret: -6.0, AVG100-Regret: 42.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:55<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:55<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:54<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:55<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:55<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 4: GAMMA 0.99 - LR 0.001 - Entropy Decay True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [12:09<5:18:04,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 0, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 36.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [25:26<5:07:04,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 0, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 44.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [38:51<5:11:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 1, Reward: 10, Cumulative-Regret: 80.0, AVG100-Regret: 33.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [53:04<4:58:31,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 1, Reward: 6, Cumulative-Regret: 84.0, AVG100-Regret: 36.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:07:16<4:48:37,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 1, Reward: 11, Cumulative-Regret: 79.0, AVG100-Regret: 42.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:20:44<4:10:25,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 1, Reward: 85, Cumulative-Regret: 5.0, AVG100-Regret: 35.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:34:02<3:58:33,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 0, Reward: 7, Cumulative-Regret: 83.0, AVG100-Regret: 34.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:48:01<4:02:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 0, Reward: 15, Cumulative-Regret: 75.0, AVG100-Regret: 39.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [2:02:10<3:33:08,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 0, Reward: 7, Cumulative-Regret: 83.0, AVG100-Regret: 44.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:15:39<3:22:17,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 1, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 40.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:29:03<3:07:23,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 1, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 39.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:43:05<3:05:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 39.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [2:56:56<2:41:41,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 1, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 36.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [3:10:18<2:27:08,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 0, Reward: 13, Cumulative-Regret: 77.0, AVG100-Regret: 39.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:23:38<2:11:54,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 0, Reward: 6, Cumulative-Regret: 84.0, AVG100-Regret: 44.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:37:22<2:07:07,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 35.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [3:51:34<1:46:22,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 0, Reward: 9, Cumulative-Regret: 81.0, AVG100-Regret: 43.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [4:04:54<1:33:18,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 40.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [4:18:15<1:19:50,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 40.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:31:38<1:06:34,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 1, Reward: 85, Cumulative-Regret: 5.0, AVG100-Regret: 40.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:44:51<53:12,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 0, Reward: 8, Cumulative-Regret: 82.0, AVG100-Regret: 39.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [4:58:55<41:54,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 1, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 31.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [5:12:28<26:19,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 38.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [5:28:05<14:46,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 1, Reward: 10, Cumulative-Regret: 80.0, AVG100-Regret: 39.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:42:21<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 43.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:00<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:00<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 5: GAMMA 0.99 - LR 0.001 - Entropy Decay False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [13:11<5:47:25,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 0, Reward: 11, Cumulative-Regret: 79.0, AVG100-Regret: 40.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [27:21<5:23:40,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 1, Reward: 95, Cumulative-Regret: -5.0, AVG100-Regret: 37.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [41:33<5:10:27,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 0, Reward: 12, Cumulative-Regret: 78.0, AVG100-Regret: 49.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [55:43<4:56:43,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 0, Reward: 13, Cumulative-Regret: 77.0, AVG100-Regret: 42.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:09:54<4:40:51,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 0, Reward: 7, Cumulative-Regret: 83.0, AVG100-Regret: 40.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:24:04<4:29:10,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 43.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:38:16<4:14:59,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 0, Reward: 10, Cumulative-Regret: 80.0, AVG100-Regret: 38.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:52:27<3:59:06,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 1, Reward: 98, Cumulative-Regret: -8.0, AVG100-Regret: 42.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [2:06:46<3:45:47,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 35.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:20:59<3:35:24,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 41.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:35:19<3:17:12,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 0, Reward: 14, Cumulative-Regret: 76.0, AVG100-Regret: 33.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:49:27<3:02:47,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 1, Reward: 96, Cumulative-Regret: -6.0, AVG100-Regret: 43.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [3:03:37<2:49:10,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 1, Reward: 93, Cumulative-Regret: -3.0, AVG100-Regret: 37.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [3:17:46<2:39:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 1, Reward: 9, Cumulative-Regret: 81.0, AVG100-Regret: 48.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:31:54<2:19:36,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 1, Reward: 4, Cumulative-Regret: 86.0, AVG100-Regret: 39.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:46:06<2:09:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 1, Reward: 12, Cumulative-Regret: 78.0, AVG100-Regret: 35.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [4:00:26<1:52:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 1, Reward: 11, Cumulative-Regret: 79.0, AVG100-Regret: 38.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [4:14:37<1:38:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 0, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 31.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [4:28:57<1:27:23,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 1, Reward: 8, Cumulative-Regret: 82.0, AVG100-Regret: 42.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:43:15<1:11:28,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 0, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 42.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:57:23<56:21,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 0, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 38.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [5:11:49<44:22,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 1, Reward: 90, Cumulative-Regret: 0.0, AVG100-Regret: 43.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [5:26:02<28:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 0, Reward: 9, Cumulative-Regret: 81.0, AVG100-Regret: 38.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [5:40:09<14:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 0, Reward: 10, Cumulative-Regret: 80.0, AVG100-Regret: 40.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:54:17<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 37.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:00<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:59<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:59<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DIFFICULTY easy\n",
      "Hyperparameter 6: GAMMA 0.99 - LR 0.0001 - Entropy Decay False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1000/25000 [12:30<5:07:52,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1000/25000, Opt. Action: 0, Reward: 8, Cumulative-Regret: 82.0, AVG100-Regret: 45.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25000 [26:26<5:22:24,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2000/25000, Opt. Action: 0, Reward: 11, Cumulative-Regret: 79.0, AVG100-Regret: 34.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3000/25000 [40:33<5:07:41,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 43.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4000/25000 [54:40<5:02:24,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 35.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5000/25000 [1:08:48<4:42:19,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5000/25000, Opt. Action: 0, Reward: 12, Cumulative-Regret: 78.0, AVG100-Regret: 36.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6000/25000 [1:22:56<4:26:35,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 31.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7000/25000 [1:37:05<4:16:51,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7000/25000, Opt. Action: 0, Reward: 8, Cumulative-Regret: 82.0, AVG100-Regret: 42.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8000/25000 [1:51:13<3:59:08,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 36.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9000/25000 [2:05:21<3:47:23,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9000/25000, Opt. Action: 1, Reward: 88, Cumulative-Regret: 2.0, AVG100-Regret: 37.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10000/25000 [2:19:44<3:37:41,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10000/25000, Opt. Action: 0, Reward: 3, Cumulative-Regret: 87.0, AVG100-Regret: 40.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11000/25000 [2:33:55<3:17:13,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 37.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12000/25000 [2:48:10<3:02:24,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12000/25000, Opt. Action: 1, Reward: 96, Cumulative-Regret: -6.0, AVG100-Regret: 39.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13000/25000 [3:02:19<2:49:17,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13000/25000, Opt. Action: 1, Reward: 92, Cumulative-Regret: -2.0, AVG100-Regret: 36.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14000/25000 [3:16:29<2:35:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14000/25000, Opt. Action: 0, Reward: 10, Cumulative-Regret: 80.0, AVG100-Regret: 33.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15000/25000 [3:30:41<2:23:03,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15000/25000, Opt. Action: 0, Reward: 5, Cumulative-Regret: 85.0, AVG100-Regret: 41.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16000/25000 [3:44:49<2:07:47,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 42.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17000/25000 [3:59:00<1:52:35,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17000/25000, Opt. Action: 0, Reward: 12, Cumulative-Regret: 78.0, AVG100-Regret: 40.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18000/25000 [4:13:08<1:37:45,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18000/25000, Opt. Action: 1, Reward: 87, Cumulative-Regret: 3.0, AVG100-Regret: 33.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19000/25000 [4:27:25<1:27:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19000/25000, Opt. Action: 1, Reward: 91, Cumulative-Regret: -1.0, AVG100-Regret: 29.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20000/25000 [4:41:45<1:11:24,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20000/25000, Opt. Action: 1, Reward: 83, Cumulative-Regret: 7.0, AVG100-Regret: 31.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21000/25000 [4:56:02<57:58,  1.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 21000/25000, Opt. Action: 0, Reward: 6, Cumulative-Regret: 84.0, AVG100-Regret: 33.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22000/25000 [5:10:31<43:11,  1.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 22000/25000, Opt. Action: 0, Reward: 8, Cumulative-Regret: 82.0, AVG100-Regret: 37.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23000/25000 [5:24:59<28:52,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 23000/25000, Opt. Action: 1, Reward: 89, Cumulative-Regret: 1.0, AVG100-Regret: 33.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24000/25000 [5:39:30<14:31,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 24000/25000, Opt. Action: 1, Reward: 84, Cumulative-Regret: 6.0, AVG100-Regret: 34.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [5:53:56<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 25000/25000, Opt. Action: 0, Reward: 7, Cumulative-Regret: 83.0, AVG100-Regret: 36.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST easy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST medium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST hard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST uniform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST independent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set Google Drive Folder\n",
    "google_drive_folder = './Monografia/Exp1-2/'\n",
    "os.makedirs(google_drive_folder + 'models', exist_ok=True)\n",
    "os.makedirs(google_drive_folder + 'train_stats', exist_ok=True)\n",
    "os.makedirs(google_drive_folder + 'test_stats', exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA_LIST = [0.7, 0.9, 0.99]\n",
    "LR_LIST = [0.001, 0.0001]\n",
    "ENTROPY_DECAY = [False, True]\n",
    "TRAIN_DIFFICULTY = ['easy']\n",
    "TEST_DIFFICULTY = ['easy', 'medium', 'hard', 'uniform', 'independent']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 48\n",
    "max_steps = 100\n",
    "n_episodes_train = 25000\n",
    "n_episodes_test= 300\n",
    "\n",
    "hyperparameters_done = [\n",
    "    (0.7, 0.0001, True),\n",
    "    (0.9, 0.0001, True),\n",
    "    (0.9, 0.0001, False),\n",
    "    (0.9, 0.001, True),\n",
    "    (0.7, 0.001, False),\n",
    "    (0.9, 0.001, False)\n",
    "]\n",
    "\n",
    "\n",
    "# Process\n",
    "for train_difficulty in TRAIN_DIFFICULTY:\n",
    "\n",
    "  hyperparameters = list(itertools.product(GAMMA_LIST, LR_LIST, ENTROPY_DECAY)) # 100 in paper\n",
    "  random.shuffle(hyperparameters)\n",
    "\n",
    "  for remove in hyperparameters_done:\n",
    "     hyperparameters.remove(remove)\n",
    "\n",
    "  for i, (gamma, learning_rate, entropy_decay) in enumerate(hyperparameters):\n",
    "\n",
    "      print(f'\\nTRAIN DIFFICULTY {train_difficulty}\\nHyperparameter {i+1}: GAMMA {gamma} - LR {learning_rate} - Entropy Decay {entropy_decay}')\n",
    "\n",
    "      # Environment and agent setup\n",
    "      env = BanditEnvironment(difficulty = train_difficulty, max_steps = max_steps)\n",
    "      num_actions = env.num_actions\n",
    "      input_dim = 1 + num_actions + 1  # Timestep, past action oh, past reward\n",
    "      agent = A2C_LSTM_Agent(input_dim, hidden_size, num_actions).to(device)\n",
    "      optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "      # Train\n",
    "      stats = train_agent(TRAIN = True, n_episodes = n_episodes_train, gamma = gamma, learning_rate = learning_rate, entropy_decay = entropy_decay)\n",
    "\n",
    "      # Save Model and Train Stats\n",
    "      agent.eval()\n",
    "      model_name = f'TrainDiff_{train_difficulty}__LR_{str(learning_rate).replace(\".\",\"_\")}__GAMMA_{str(gamma).replace(\".\",\"_\")}__EntropyDecay_{str(entropy_decay)}'\n",
    "      torch.save(agent.state_dict(), google_drive_folder + 'models/'+ model_name + '.pth')\n",
    "      save_train_stats(stats, model_name, train_difficulty, gamma, learning_rate, entropy_decay)\n",
    "\n",
    "      for test_difficulty in TEST_DIFFICULTY:\n",
    "\n",
    "          print(f'\\nTEST {test_difficulty}:')\n",
    "\n",
    "          # Test and save stats\n",
    "          stats = train_agent(TRAIN = False, n_episodes = n_episodes_test, gamma = gamma, learning_rate = learning_rate, entropy_decay = entropy_decay)\n",
    "          save_test_stats(stats, model_name, train_difficulty, test_difficulty, gamma, learning_rate, entropy_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
